import * as vscode from 'vscode';
import { Logger } from '../../utils/logger';

/**
 * ä¸Šä¸‹æ–‡çª—å£ç®¡ç†å™¨ - è´Ÿè´£åŠ¨æ€ä¸Šä¸‹æ–‡çª—å£é…ç½®å’Œç®¡ç†
 */
export class ContextWindowManager {
  private logger = Logger.getInstance();
  
  // åŠ¨æ€æ¨¡å‹é…ç½®ç¼“å­˜
  private static modelConfigCache = new Map<string, {
    maxTokens: number;
    warningThreshold: number;
    compressionThreshold: number;
    lastUpdated: number;
    confidence: 'low' | 'medium' | 'high';
  }>();

  /**
   * ğŸš€ åŠ¨æ€ä¸Šä¸‹æ–‡çª—å£é…ç½®ï¼šå¤šå±‚æ¬¡è‡ªé€‚åº”ç­–ç•¥
   */
  public async getContextWindowConfig(selectedModel: vscode.LanguageModelChat): Promise<{
    maxTokens: number;
    warningThreshold: number;
    compressionThreshold: number;
  }> {
    const modelKey = selectedModel.name;
    
    // 1. ä¼˜å…ˆçº§1ï¼šç”¨æˆ·é…ç½®è¦†ç›–
    const userConfig = await this.loadUserModelConfig(modelKey);
    if (userConfig) {
      const config = {
        maxTokens: userConfig.maxTokens || 8000,
        warningThreshold: userConfig.warningThreshold || Math.floor((userConfig.maxTokens || 8000) * 0.75),
        compressionThreshold: userConfig.compressionThreshold || Math.floor((userConfig.maxTokens || 8000) * 0.6)
      };
      this.logger.info(`ğŸ‘¤ Using user config for ${modelKey}: ${config.maxTokens} tokens`);
      return config;
    }
    
    // 2. ä¼˜å…ˆçº§2ï¼šé”™è¯¯å­¦ä¹ ç¼“å­˜ï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰
    const cached = ContextWindowManager.modelConfigCache.get(modelKey);
    if (cached && cached.confidence === 'high') {
      this.logger.info(`ğŸ¯ Using learned config for ${modelKey}: ${cached.maxTokens} tokens (high confidence)`);
      return {
        maxTokens: cached.maxTokens,
        warningThreshold: cached.warningThreshold,
        compressionThreshold: cached.compressionThreshold
      };
    }
    
    // 3. ä¼˜å…ˆçº§3ï¼šæ™®é€šç¼“å­˜ï¼ˆ24å°æ—¶æœ‰æ•ˆï¼‰
    if (cached && (Date.now() - cached.lastUpdated) < 24 * 60 * 60 * 1000) {
      this.logger.info(`ğŸ“‹ Using cached config for ${modelKey} (confidence: ${cached.confidence})`);
      return {
        maxTokens: cached.maxTokens,
        warningThreshold: cached.warningThreshold,
        compressionThreshold: cached.compressionThreshold
      };
    }
    
    // 4. ä¼˜å…ˆçº§4ï¼šå¯å‘å¼æ¨æ–­ï¼ˆåŸºäºé€šç”¨è§„åˆ™ï¼‰
    const inferredConfig = this.inferModelCapabilities(selectedModel);
    
    // 5. ç¼“å­˜æ¨æ–­ç»“æœ
    ContextWindowManager.modelConfigCache.set(modelKey, {
      ...inferredConfig,
      lastUpdated: Date.now(),
      confidence: 'medium'
    });
    
    this.logger.info(`ğŸ” Inferred config for ${modelKey}: ${inferredConfig.maxTokens} tokens (medium confidence)`);
    return {
      maxTokens: inferredConfig.maxTokens,
      warningThreshold: inferredConfig.warningThreshold,
      compressionThreshold: inferredConfig.compressionThreshold
    };
  }

  /**
   * ğŸš€ å¯å‘å¼æ¨¡å‹èƒ½åŠ›æ¨æ–­ï¼šåŸºäºåç§°æ¨¡å¼å’Œé€šç”¨è§„åˆ™
   */
  private inferModelCapabilities(model: vscode.LanguageModelChat) {
    const modelName = model.name.toLowerCase();
    
    // æ£€æŸ¥æ˜¯å¦æ˜¯æ–°ä¸€ä»£å¤§ä¸Šä¸‹æ–‡æ¨¡å‹
    if (this.isLargeContextModel(modelName)) {
      return { maxTokens: 128000, warningThreshold: 100000, compressionThreshold: 80000 };
    }
    
    // æ£€æŸ¥æ˜¯å¦æ˜¯ä¸­ç­‰ä¸Šä¸‹æ–‡æ¨¡å‹  
    if (this.isMediumContextModel(modelName)) {
      return { maxTokens: 32000, warningThreshold: 25000, compressionThreshold: 20000 };
    }
    
    // æ£€æŸ¥æ˜¯å¦æ˜¯è€æ¨¡å‹æˆ–å°æ¨¡å‹
    if (this.isSmallContextModel(modelName)) {
      return { maxTokens: 4000, warningThreshold: 3000, compressionThreshold: 2000 };
    }
    
    // é»˜è®¤ä¿å®ˆä¼°è®¡
    return { maxTokens: 8000, warningThreshold: 6000, compressionThreshold: 4000 };
  }

  /**
   * æ£€æŸ¥æ˜¯å¦æ˜¯å¤§ä¸Šä¸‹æ–‡æ¨¡å‹
   */
  private isLargeContextModel(modelName: string): boolean {
    const largeContextIndicators = [
      'turbo', '128k', '200k', 'long', 'extended',
      'claude-3', 'claude-2.1', 'gemini-pro',
      '2024', '2023'  // è¾ƒæ–°çš„æ¨¡å‹é€šå¸¸æœ‰æ›´å¤§çš„ä¸Šä¸‹æ–‡
    ];
    return largeContextIndicators.some(indicator => modelName.includes(indicator));
  }

  /**
   * æ£€æŸ¥æ˜¯å¦æ˜¯ä¸­ç­‰ä¸Šä¸‹æ–‡æ¨¡å‹
   */
  private isMediumContextModel(modelName: string): boolean {
    const mediumContextIndicators = [
      'gpt-4', 'claude-2', 'gemini',
      '16k', '32k'
    ];
    return mediumContextIndicators.some(indicator => modelName.includes(indicator));
  }

  /**
   * æ£€æŸ¥æ˜¯å¦æ˜¯å°ä¸Šä¸‹æ–‡æ¨¡å‹
   */
  private isSmallContextModel(modelName: string): boolean {
    const smallContextIndicators = [
      'gpt-3.5', '4k', '2k', 
      '2022', '2021'  // è¾ƒè€çš„æ¨¡å‹
    ];
    return smallContextIndicators.some(indicator => modelName.includes(indicator));
  }

  /**
   * ğŸš€ Tokenæ•°é‡ä¼°ç®—ï¼šç®€å•ä½†æœ‰æ•ˆçš„ä¼°ç®—æ–¹æ³•
   */
  public estimateTokens(text: string): number {
    // ç®€å•ä¼°ç®—ï¼š1 token â‰ˆ 0.75 è‹±æ–‡å•è¯ â‰ˆ 4 å­—ç¬¦
    // å¯¹äºä¸­æ–‡ï¼Œ1ä¸ªå­—ç¬¦å¤§çº¦ç­‰äº1ä¸ªtoken
    const chineseChars = (text.match(/[\u4e00-\u9fff]/g) || []).length;
    const englishWords = text.replace(/[\u4e00-\u9fff]/g, '').split(/\s+/).filter(w => w.length > 0).length;
    
    return Math.ceil(chineseChars + englishWords * 1.3);
  }

  /**
   * ğŸš€ å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†ï¼šæ™ºèƒ½å‹ç¼©å†å²è®°å½• + é”™è¯¯åé¦ˆå­¦ä¹ 
   */
  public async manageConversationContext(
    conversationHistory: Array<{ role: string; content: string; tokens?: number; toolResults?: any[] }>,
    contextConfig: { maxTokens: number; warningThreshold: number; compressionThreshold: number },
    selectedModel: vscode.LanguageModelChat
  ): Promise<void> {
    const totalTokens = conversationHistory.reduce((sum, item) => sum + (item.tokens || 0), 0);
    
    if (totalTokens <= contextConfig.compressionThreshold) {
      return; // æ— éœ€å‹ç¼©
    }
    
    this.logger.warn(`ğŸ’­ Context approaching limit (${totalTokens}/${contextConfig.maxTokens} tokens), compressing history`);
    
    // ä¿ç•™æœ€æ–°çš„2è½®å¯¹è¯å’Œåˆå§‹ç”¨æˆ·è¾“å…¥
    const originalUserInput = conversationHistory[0];
    const recentHistory = conversationHistory.slice(-4); // æœ€è¿‘2è½®å¯¹è¯
    
    // å‹ç¼©ä¸­é—´å†å²ä¸ºæ‘˜è¦
    const middleHistory = conversationHistory.slice(1, -4);
    if (middleHistory.length > 0) {
      const compressionSummary = await this.compressHistoryToSummary(middleHistory, selectedModel);
      
      // é‡æ„å¯¹è¯å†å²
      conversationHistory.length = 0; // æ¸…ç©ºæ•°ç»„
      conversationHistory.push(originalUserInput);
      
      if (compressionSummary) {
        conversationHistory.push({
          role: 'system',
          ontent: `ğŸ“‹ **History Summary**: ${compressionSummary}`,
          tokens: this.estimateTokens(compressionSummary)
        });
      }
      
      conversationHistory.push(...recentHistory);
      
      const newTotalTokens = conversationHistory.reduce((sum, item) => sum + (item.tokens || 0), 0);
      this.logger.info(`âœ… Context compressed: ${totalTokens} â†’ ${newTotalTokens} tokens`);
    }
  }

  /**
   * ğŸš€ é”™è¯¯åé¦ˆå­¦ä¹ ï¼šä»å®é™…APIé”™è¯¯ä¸­å­¦ä¹ æ¨¡å‹é™åˆ¶
   */
  public async handleContextError(
    error: Error,
    selectedModel: vscode.LanguageModelChat,
    estimatedTokens: number
  ): Promise<void> {
    const errorMessage = error.message.toLowerCase();
    
    // æ£€æŸ¥æ˜¯å¦æ˜¯ä¸Šä¸‹æ–‡è¶…é™é”™è¯¯
    if (this.isContextLimitError(errorMessage)) {
      const modelKey = selectedModel.name;
      const cached = ContextWindowManager.modelConfigCache.get(modelKey);
      
      if (cached) {
        // æ ¹æ®é”™è¯¯è°ƒæ•´é…ç½®ï¼ˆä¿å®ˆé™ä½ï¼‰
        const newMaxTokens = Math.floor(estimatedTokens * 0.8); // é™ä½20%
        const updatedConfig = {
          maxTokens: newMaxTokens,
          warningThreshold: Math.floor(newMaxTokens * 0.8),
          compressionThreshold: Math.floor(newMaxTokens * 0.6),
          lastUpdated: Date.now(),
          confidence: 'high' as const // ä»å®é™…é”™è¯¯å­¦ä¹ ï¼Œç½®ä¿¡åº¦æœ€é«˜
        };
        
        ContextWindowManager.modelConfigCache.set(modelKey, updatedConfig);
        
        this.logger.warn(`ğŸ”§ Learned from context error for ${modelKey}: ${cached.maxTokens} â†’ ${newMaxTokens} tokens`);
      }
    }
  }

  /**
   * æ£€æŸ¥é”™è¯¯æ˜¯å¦ä¸ºä¸Šä¸‹æ–‡é™åˆ¶é”™è¯¯
   */
  private isContextLimitError(errorMessage: string): boolean {
    const contextErrorIndicators = [
      'context length',
      'token limit',
      'maximum context',
      'too long',
      'context size',
      '4096', '8192', '16384', '32768'
    ];
    return contextErrorIndicators.some(indicator => errorMessage.includes(indicator));
  }

  /**
   * ğŸš€ ç”¨æˆ·é…ç½®è¦†ç›–ï¼šå…è®¸ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹é…ç½®
   */
  private async loadUserModelConfig(modelName: string): Promise<{
    maxTokens?: number;
    warningThreshold?: number;
    compressionThreshold?: number;
  } | null> {
    try {
      const config = vscode.workspace.getConfiguration('srs-writer');
      const userModelConfigs = config.get<Record<string, any>>('modelConfigs', {});
      
      const userConfig = userModelConfigs[modelName];
      if (userConfig && typeof userConfig === 'object') {
        this.logger.info(`ğŸ‘¤ Using user-defined config for ${modelName}`);
        return userConfig;
      }
    } catch (error) {
      this.logger.warn(`Failed to load user model config: ${(error as Error).message}`);
    }
    return null;
  }

  /**
   * ğŸš€ å†å²å‹ç¼©ï¼šå°†å¤šè½®å¯¹è¯å‹ç¼©ä¸ºç®€æ´æ‘˜è¦
   */
  private async compressHistoryToSummary(
    historyToCompress: Array<{ role: string; content: string; toolResults?: any[] }>,
    selectedModel: vscode.LanguageModelChat
  ): Promise<string | null> {
    try {
      // æ„å»ºå‹ç¼©æç¤ºè¯
      const historyText = historyToCompress.map((item, index) => {
        if (item.toolResults) {
          const successCount = item.toolResults.filter(r => r.success).length;
          const toolNames = item.toolResults.map(r => r.toolName).join(', ');
         return `Step ${index + 1}: Execute tool[${toolNames}] - ${successCount}/${item.toolResults.length} successful`;
        }
        return `${item.role}: ${item.content.substring(0, 100)}...`;
      }).join('\n');

      const compressionPrompt = `è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²å‹ç¼©ä¸º1-2å¥è¯çš„ç®€æ´æ‘˜è¦ï¼Œé‡ç‚¹å…³æ³¨ï¼š
1. å®Œæˆäº†å“ªäº›å…³é”®æ“ä½œ
2. æ˜¯å¦æœ‰å¤±è´¥éœ€è¦ä¿®æ­£
3. å½“å‰è¿›å±•çŠ¶æ€

å†å²è®°å½•ï¼š
${historyText}

æ‘˜è¦ï¼ˆ1-2å¥è¯ï¼‰ï¼š`;

      const messages = [vscode.LanguageModelChatMessage.User(compressionPrompt)];
      const response = await selectedModel.sendRequest(messages, {
        justification: 'Compress conversation history for context management'
      });

      let summary = '';
      for await (const fragment of response.text) {
        summary += fragment;
      }

      return summary.trim();
    } catch (error) {
      this.logger.warn(`Failed to compress history: ${(error as Error).message}`);
      return null;
    }
  }
} 
